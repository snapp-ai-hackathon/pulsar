Pulsar – ارائه فنی و بیزنسی (تمرکز روی ML/AI)
=============================================

> **Pulsar یک کوپایلوت هوش مصنوعی است** برای راننده‌های اسنپ که با استفاده از مدل‌های زمان‑سری و شبکه‌های عصبی، موج‌های تقاضا و سرج را چند گام جلوتر می‌بیند و به راننده و تیم اوپس می‌گوید «الان کجا برو، چه زمانی جابه‌جا شو و این تصمیم چه اثری روی درآمد و تجربه‌ی مسافر دارد».

در این ارائه می‌خواهیم نشان بدهیم که هسته‌ی ارزش Pulsar از **سه لایه‌ی AI** می‌آید:

1. **Feature Engineering زمان‑سری** روی داده‌های واقعی سفرها و سرج؛  
2. **مدل‌های پیش‌بینی** شامل ElasticNet (قابل توضیح)، و یک Temporal CNN برای پیش‌بینی سرج در افق‌های چندگانه؛  
3. **زیرساخت MLOps هوشمند** شامل MLflow و NATS که این مدل‌ها را به یک سیستم قابل استقرار روی ناوگان واقعی تبدیل می‌کند.

بقیه‌ی لایه‌ها (ClickHouse، فایل‌های CSV، Docker، Helm) فقط این مغز ML را به یک محصول صنعتی تبدیل می‌کنند.  
این سند را می‌توان هم به عنوان اسکریپت ارائه، هم به عنوان مستند فنی برای تیم داوری و تیم‌های فنی اسنپ استفاده کرد.

---

## 1. داستان کلی در چند جمله (اسلاید اول)

1. **مسئله کسب‌وکاری**  
   - راننده‌ها از «زمان مُرده» و سرج‌های غیرقابل‌پیش‌بینی ناراضی‌اند.  
   - مسافرها از این‌که هر بار اپ را باز می‌کنند با سرج عجیب روبه‌رو شوند خسته شده‌اند.  
   - تیم‌های عملیاتی مجبورند مدام دستی آتش‌نشانی کنند: قیمت را بالا/پایین ببرند، کمپین بزنند، نقشه را تنظیم کنند.

2. **پاسخ ما: Pulsar**  
   - Pulsar یک موتور پیش‌بینی **ساعت بعدی** تقاضا و سرج است که روی شبکه هگزاگونی شهر سوار می‌شود.  
   - به جای این‌که بعد از وقوع بحران واکنش نشان دهیم، قبل از شلوغی، سیگنال می‌دهیم:
     - «در ۳۰ دقیقه‌ی بعدی در این محدوده سرج بالا می‌رود، راننده‌ها را بچرخان.»  
     - «اینجا الان خیلی شلوغ است، کمپین را این‌جا فعال نکن.»

3. **چرا الان قابل اجراست؟**  
   - داده‌ی واقعی از `ClickHouse` و CSV داریم.  
   - لایه‌ی `TimeSeriesStore` و مدل‌های ML (`ElasticNet`, `TemporalCNN`) پیاده‌سازی شده‌اند.  
   - با `NATS` می‌توانیم *آموزش مدل* را رویدادمحور کنیم و در آینده با زیرساخت اصلی اسنپ یکپارچه شویم.

این سه محور را می‌توان در ۳۰–۴۵ ثانیه اول ارائه گفت تا ذهن همه روی «مسئله → راه‌حل → امکان‌پذیری» قفل شود.

---

## 2. معماری داده و مسیر کلی (از CSV تا مدل)

### 2.1 جریان داده

1. **ورود داده**
   - داده‌های آفلاین از طریق فایل‌های `data/*.csv` و خروجی جدول `kandoo_parameter_nats` روی `ClickHouse` وارد سیستم می‌شود.
   - هر ردیف شامل است:
     - زمان (`clickhouse_time`)
     - شناسه هگزا (`hex_id`)
     - `service_type`, `city_id`
     - سیگنال‌های سرج: `surge_percent`, `surge_absolute`, `cumulative_surge_*`
     - سیگنال‌های رفتاری: `accept_rate`, `price_cnvr` و …

2. **ساخت Snapshot**
   - کلاس `CSVSignalLoader` این داده‌ها را می‌خواند و به همراه `SnapshotBuilder`، برای هر `(hexagon, service_type, city_id, period)` یک شیء `HexagonSnapshot` می‌سازد.
   - این snapshot شامل ویژگی‌هایی مثل:
     - `acceptance_rate`, `price_conversion`, `demand_signal`, `supply_signal`
     - مقادیر سرج و cumulative surge.

3. **ذخیره‌سازی در TimeSeriesStore**
   - کلاس `TimeSeriesStore` این اسنپ‌شات‌ها را به صورت ردیف‌های زمانی در فایل‌های پارکت (`cache/timeseries/{hexagon}_{service_type}.parquet`) ذخیره می‌کند.
   - هر فایل در واقع تاریخچه‌ی یک هگزا–سرویس است.

4. **ساخت دیتاست برای مدل‌ها**
   - برای **ElasticNet** (کلاس `MLTrainer`):
     - همه پارکت‌ها با هم `concat` می‌شوند.  
     - برای هر هگزا/سرویس، lag یک‑گانه از `demand_signal`, `supply_signal`, `acceptance_rate` ساخته می‌شود (`lag_demand`, `lag_supply`, `lag_acceptance`).  
     - تارگت: `demand_signal` در گام بعدی است.
   - برای **Temporal CNN** (کلاس `CNNTrainer`):
     - روی هر پارکت، پنجره‌های طولی `window_size` (پیش‌فرض ۱۲ بازه‌ی زمانی ~ مثلاً ۱۲×۷.۵ دقیقه) ساخته می‌شود.  
     - ورودی: ماتریس `[window_size × feature_dim]` شامل ۸ ویژگی اصلی (در `feature_columns`).  
     - خروجی: یک بردار از مقدار سرج آینده در چند افق مختلف (`forecast.horizons` مثلاً ۳۰ و ۶۰ و ۹۰ دقیقه).

5. **آموزش مدل و لاگ در MLflow**
   - پس از ساخت دیتاست، مدل‌ها روی GPU/CPU آموزش داده می‌شوند، متریک‌ها (`mae`, `rmse`) محاسبه و در MLflow با پارامترها و وزن‌های مدل ثبت می‌شوند.

این بخش را می‌توان در یک اسلاید «Data Flow» با فلش نشان داد و شفاهی توضیح داد که: «ما روی داده‌های آماده نمی‌نشینیم؛ خودمان لایه‌ی زمان‑سری را می‌سازیم.»

---

## 3. لایه‌های هوش مصنوعی در Pulsar (AI Stack)

برای این‌که در ارائه هم دانشگاهی قوی باشی، هم بیزنسی، خوب است AI را به چند لایه‌ی مشخص بشکنی و روی هر کدام ۲۰–۳۰ ثانیه مانور بدهی:

1. **لایه‌ی Feature Engineering زمان‑سری**
   - تبدیل سیگنال‌های خام به ویژگی‌های معنادار:
     - lagهای مختلف (۱ تیک قبل، چند تیک قبل)،
     - سیگنال‌های تجمیعی (`demand_signal`, `supply_signal`)،
     - cumulative surge و نسبت‌های رفتاری (`price_conversion`).
   - این‌جا در واقع همان کاری را می‌کنیم که یک Data Scientist دستی انجام می‌داد، ولی اتوماتیک و پایدار.

2. **لایه‌ی مدل‌های کلاسیک (ElasticNet)**
   - مدل خطی با regularization که پایه‌ی قابل توضیح و سریع ماست.
   - به‌خاطر وزن‌های روشن، این لایه بخش «Explainable AI» داستان ماست.

3. **لایه‌ی مدل‌های عمیق (Temporal CNN و در آینده LSTM/TFN)**
   - CNN روی پنجره‌ی زمانی، الگوهای پیچیده‌تری را می‌بیند که خطی نیستند (مثلاً چند قله و دره‌ی پشت‌سرهم قبل از سرج).
   - در Roadmap، LSTM/TFN برای مدل‌کردن توالی‌های طولانی‌تر و ترکیب چند نوع فیچر (آب‌وهوا، event، ترافیک) اضافه می‌شوند.

4. **لایه‌ی MLOps (MLflow + NATS)**
   - MLflow همه‌ی این تجربه‌ها را ثبت و قابل بازپخش می‌کند.
   - NATS به ما اجازه می‌دهد *هوش* را به صورت event‑driven وارد بقیه‌ی سرویس‌ها کنیم (هر جا که لازم باشد، فقط یک پیام بفرست و مدل آموزش ببیند).

با این framing، وقتی می‌گویی «ما از AI استفاده کردیم»، دقیقاً می‌توانی نشان بدهی AI در کجاها نشسته است، نه فقط در یک اسلاید مبهم.

---

## 4. مدل‌های فعلی: ElasticNet و CNN

### 4.1 ElasticNet – لایه‌ی ML کلاسیک، قابل توضیح

**چرا اول سراغ ElasticNet رفتیم؟**

- بسیار سریع است؛ برای چند صد هزار ردیف در چند ثانیه آموزش می‌بیند.
- به‌شدت **قابل توضیح** است:
  - هر ضریب (`coefficient`) دقیقاً می‌گوید «اگر `lag_demand` ۱ واحد بیشتر شود، پیش‌بینی demand چقدر زیاد می‌شود».
- برای شروع، فهمیدن رفتار داده و validate کردن pipeline فوق‌العاده است.

**ساختار مدل (`MLTrainer`):**

- ورودی: `[lag_demand, lag_supply, lag_acceptance, price_conversion]`
- مدل: `ElasticNet(alpha=0.3, l1_ratio=0.1)`  
- خروجی: یک اسکالر `demand_signal` در بازه‌ی بعدی.
- آموزش:
  - `train_test_split` (۸۰٪/۲۰٪)
  - متریک‌ها: MAE و RMSE
  - لاگ در MLflow با نام `elasticnet-demand-<date>`.

در ارائه‌ی فنی می‌توانی بگویی:

> «ElasticNet برای ما مثل baseline است؛ هم سریع است، هم شفاف است، و اگر فردا کسی از تیم قیمت‌گذاری بپرسد *چرا این اتفاق افتاد؟* می‌توانیم نمودار ضرایب را نشان بدهیم.»

---

### 4.2 Temporal CNN – شبکه‌ی عصبی برای پیش‌بینی سرج

**ایده‌ی اصلی:**

- به‌جای این‌که فقط به آخرین نقطه‌ی زمان‌سری نگاه کنیم، یک **پنجره‌ی زمانی** از ۱۲ بازه‌ی اخیر را می‌گیریم (مثلاً ۱۲×۷.۵ دقیقه = ~۹۰ دقیقه‌ی گذشته).  
- برای هر هگزا و سرویس، یک دنباله‌ی `window_size × feature_dim` می‌سازیم؛ هر ردیف ترکیبی از:
  - `acceptance_rate`, `price_conversion`, `demand_signal`, `supply_signal`,
  - `surge_percent`, `surge_absolute`,
  - `cumulative_surge_percent`, `cumulative_surge_absolute`.
- CNN روی محور زمان یک فیلتر ۱ بعدی می‌کشد و الگوهایی شبیه «سه بازه‌ی رشد سریع + افت supply» را شناسایی می‌کند.

**ساختار دقیق `TemporalCNN`:**

- ورودی: `x` با شکل `(batch, window, features)`.
- لایه‌ها:
  1. `Conv1d(in_channels = feature_dim, out_channels = 64, kernel_size = 3, padding = 1)`  
  2. `ReLU`
  3. `Conv1d(64 → 64, kernel_size = 3, padding = 1)`
  4. `ReLU`
  5. `AdaptiveAvgPool1d(1)` → فشرده‌سازی بعد زمانی
  6. `Dropout(p=0.15)` برای جلوگیری از overfitting
  7. `Linear(64 → target_dim)`؛ `target_dim` = تعداد افق‌های پیش‌بینی (مثلاً ۲ یا ۳ خروجی: ۳۰ / ۶۰ / ۹۰ دقیقه)

به زبان ساده در ارائه:

> «ما ۱۲ تیک آخر هر هگزا را مثل یک سیگنال صوتی در نظر می‌گیریم، روی آن فیلتر زمانی می‌کشیم تا الگوهای قبل از سرج را ببینیم، و بعد با یک لایه‌ی نهایی، مقدار سرج آینده را برای چند تایم‌فریم مختلف حدس می‌زنیم.»

**فرآیند آموزش CNN (`CNNTrainer.train`):**

- **ساخت توالی‌ها**:
  - از هر فایل پارکت، اگر حداقل `(window_size + max_step + 1)` ردیف داشته باشد، هزاران جفت `(sequence, target)` می‌سازیم.
  - `sequence`: ماتریس ۱۲×۸ (برای ۸ ویژگی اصلی).
  - `target`: برداری از مقادیر `surge_percent` در گام‌های آینده (بر اساس `forecast.horizons` و `period_duration_minutes` محاسبه شده).
- **ساخت `DataLoader`**:
  - `SequenceWindowDataset` برای پکیج‌کردن `inputs` و `targets` در قالب `torch.Tensor`.
  - `random_split` به train/val.
- **حلقه‌ی آموزش**:
  - optimizer: `Adam`, loss: `MSELoss`.
  - چاپ `train_loss` و `val_loss` در هر epoch.
- **ارزیابی و لاگ در MLflow**:
  - محاسبه‌ی MAE و RMSE روی کل داده.
  - `mlflow.pytorch.log_model` برای ذخیره‌ی مدل CNN.

این باعث می‌شود که CNN به صورت عملیاتی و قابل تست در کنار ElasticNet قرار بگیرد و آماده‌ی A/B تست روی ترافیک واقعی باشد؛ و از دید داور/استاد، این‌جا دقیقاً جایی است که «هوش مصنوعی» فراتر از رگرسیون ساده وارد بازی می‌شود.

---

## 5. نقش MLflow در چرخه‌ی عمر مدل (MLOps هوشمند)

MLflow ستون فقرات **رصد و مدیریت مدل‌ها** در Pulsar است:

- هر بار که `MLTrainer.train` یا `CNNTrainer.train` اجرا می‌شود:
  - یک `run` جدید در MLflow ایجاد می‌شود.
  - پارامترها (مثل `alpha`, `l1_ratio`, `window_size`, `epochs`, `horizons`) ذخیره می‌شوند.
  - متریک‌ها (`mae`, `rmse`, تعداد ردیف‌ها، تعداد هگزاها) ثبت می‌شوند.
  - مدل نهایی به عنوان artifact (`model`) ذخیره می‌شود و `model_uri` تولید می‌شود.

**مزایا:**

- **Traceability**: می‌دانیم کدام نسخه‌ی مدل روی کدام تاریخ آموزش دیده و چه عملکردی داشته.
- **مقایسه‌ی سناریوها**: مثلاً می‌توانیم مقایسه کنیم:
  - ElasticNet vs CNN
  - افق ۳۰ دقیقه vs ۶۰ دقیقه
  - با/بدون ویژگی‌های جدید (مثلاً آب‌وهوا).
- **آماده برای MLOps اسنپ**: تیم‌های دیگر که قبلاً با MLflow کار می‌کنند، می‌توانند مدل‌های Pulsar را به‌راحتی لود و در سرویس‌های خودشان استفاده کنند.

در ارائه دانشگاهی می‌توانی روی این تأکید کنی که:

> «ما فقط مدل نساختیم، بلکه چرخه‌ی کامل *Data → Model → Metrics → Registry* را هم در نظر گرفتیم، مطابق best practiceهای MLOps.»

---

## 6. NATS – چرا و کجا در تصویر می‌آید؟

در نسخه‌ی فعلی، NATS را برای **ارکستریشن آموزش آنلاین** استفاده می‌کنیم، نه برای سروینگ بلادرنگ.

### 5.1 الگوی فعلی

- یک سرویس خارجی (مثلاً ClickHouse exporter یا سرویس اپراتوری) یک پیام روی subject مثلاً `kandoo.parameter.train` منتشر می‌کند که در آن مشخص است:
  - `model_type`: `"elasticnet"` یا `"cnn"`.
  - `service_types`: لیست سرویس‌ها (مثلاً `[1,2,3,4,5,6,7,8,24,31]`).
  - `alpha`, `l1_ratio`, `train_date`, `force` و …
- مصرف‌کننده (در کد: `NATSTrainerConsumer`) این پیام را می‌گیرد،:
  - دیتای CSV/ClickHouse را برای بازه‌ی مربوطه از نو بارگذاری می‌کند،
  - اسنپ‌شات‌ها و `TimeSeriesStore` را آپدیت می‌کند،
  - مدل را با کانفیگ خواسته‌شده آموزش می‌دهد،
  - نتیجه را در MLflow و لاگ‌ها ثبت می‌کند.

به این ترتیب:

> «آموزش مدل یک *event* می‌شود، نه یک job دستی. هر سیستم دیگری در اکوسیستم اسنپ می‌تواند فقط با publish کردن یک پیام، یک retrain کامل را تریگر کند.»

### 5.2 آینده‌ی NATS در محصول

- اتصال به **کلاستر NATS اصلی اسنپ** با:
  - احراز هویت و مجوز،
  - retry و idempotent job id،
  - مانیتورینگ و آلارم روی نرخ fail/retry.
- گسترش موضوعات:
  - `parameter.train` برای آموزش،
  - `parameter.forecast` برای بروزرسانی مدل‌های سرویسی دیگر،
  - و حتی `parameter.feedback` برای برگرداندن نتایج A/B تست‌ها به سیستم یادگیری.

در ارائه می‌توانی بگویی:

> «در نسخه‌ی هکاتونی، NATS را برای راه‌انداختن روال آموزش آنلاین استفاده کردیم. در نسخه‌ی پرو، همین الگو را روی کلاستر اصلی NATS می‌بریم و آن را به مرکز عصبی کل فرآیندهای ML تبدیل می‌کنیم.»

---

## 7. ایده‌های آینده: فراتر از سرج و فقط راننده

Pulsar در واقع یک **موتور پیش‌بینی فشار و ازدحام** است؛ وقتی این موتور را داشته باشیم، می‌توانیم:

1. **کنترل هوشمند ترافیک و پیشنهادهای پویا**
   - استفاده از پیش‌بینی سرج + داده‌های ترافیک برای:
     - ارسال پیشنهاد به راننده‌ها: «اگر از خیابان شلوغ خارج شوی و به این محدوده بروی، ترافیک کمتر + تقاضای بهتر می‌گیری».
     - پیشنهاد به مسافر: «اگر نقطه‌ی سوارشدنت را ۳۰۰ متر جابه‌جا کنی، هم سریع‌تر سوار می‌شوی، هم هزینه کمتر می‌شود.»

2. **هماهنگی بین سرویس‌ها (Ride, Food, Intercity, …)**
   - اگر یک هاب خیلی شلوغ است، می‌توانیم:
     - سرج سواری را بالا ببریم ولی همزمان شدت تخفیف Food را کم کنیم.
     - کمپین‌های بازاریابی را به سمت منطقه‌ای ببریم که ظرفیت خالی بیشتری دارد.

3. **قوانین عدالت و کیفیت سرویس**
   - تعریف قاعده‌هایی مثل:
     - “هیچ راننده‌ای نباید بیش از X دقیقه بدون سفر در هسته‌ی سرویس بماند.”
     - “نباید همه‌ی سرج را روی چند محله‌ی خاص قفل کنیم.”
   - این قوانین می‌توانند به شکل محدودیت در تابع هدف مدل‌های ML اعمال شوند (regularization بر حسب fairness).

این بخش را می‌توان در اسلایدهای ۷ و ۸ توضیح داد تا نشان بدهی که این کار فقط یک PoC نیست، بلکه یک **پلتفرم قابل گسترش** است.

---

## 8. سؤالات محتمل و پاسخ‌های پیشنهادی

### سؤال ۱ – چرا CNN و نه LSTM/Transformer؟

**پاسخ:**

- CNN برای الگوهای کوتاه‌مدت (مثلاً ۱–۲ ساعت اخیر) فوق‌العاده است و روی GPU بسیار سریع آموزش می‌بیند.
- معماری ساده‌تری دارد؛ دو لایه Conv1d + pooling + FC → دیباگ و دیپلوی راحت‌تر.
- LSTM/TFN را به‌عنوان مرحله‌ی بعدی در نظر گرفته‌ایم، مخصوصاً وقتی افق زمانی طولانی‌تر و تعداد فیچرها بیشتر شود؛ اما برای نسخه‌ی اول، خواستیم ریسک مدل را کنترل کنیم و روی کیفیت دیتاست و پایپ‌لاین تمرکز کنیم.

---

### سؤال ۲ – چطور مطمئن می‌شوید واقعاً درآمد/تعداد سفر را بالا می‌برد؟

**پاسخ:**

- در این نسخه هیچ درصد مشخصی وعده نمی‌دهیم؛ تمرکز ما روی **آماده‌کردن زیرساخت پیش‌بینی و آموزش سریع** است.
- وقتی به محیط واقعی وصل شویم:
  - A/B تست روی گروه‌های راننده و شهرها راه می‌افتد.
  - KPIهایی مثل:
    - تعداد سفر در بازه‌های سرج،
    - درآمد ساعتی راننده،
    - توزیع زمان انتظار مسافر  
    با قبل و بعد از فعال‌سازی Pulsar مقایسه می‌شوند.

می‌توانی بگویی:

> «الان ثابت کردیم که مدل و زیرساخت کار می‌کند؛ قدم بعدی، اندازه‌گیری اثر روی بیزینس در محیط واقعی است.»

---

### سؤال ۳ – اگر NATS یا ClickHouse مشکلی پیدا کنند چه می‌شود؟

**پاسخ:**

- کل تاریخچه‌ی مورد نیاز در `TimeSeriesStore` به صورت پارکت ذخیره می‌شود؛ یعنی اگر لحظه‌ای NATS در دسترس نباشد، داده از بین نمی‌رود.
- برای آموزش، همیشه می‌توانیم دوباره از روی `ClickHouse`/CSV بازسازی کنیم.
- در نسخه‌ی پرو:
  - برای NATS retry و بک‌آف نمایی داریم،
  - مانیتورینگ روی نرخ پیام‌های fail،
  - و jobهای دوره‌ای برای check و تکمیل gapها.

---

### سؤال ۴ – چه لایه‌هایی در ML دارید؟ (چند لایه؟)

**پاسخ (خلاصه):**

- **Linear (ElasticNet)**:
  - یک لایه‌ی خطی با ۴ ورودی و ۱ خروجی.
- **CNN**:
  - ۲ لایه‌ی `Conv1d` (هر کدام ۶۴ فیلتر، kernel=3),
  - لایه‌های `ReLU`، `AdaptiveAvgPool1d`، `Dropout(0.15)`,
  - لایه‌ی تمام‌متصل نهایی برای تولید خروجی چندبعدی (multi‑horizon).
- (در Roadmap) **LSTM/TFN**:
  - چند لایه LSTM با attention روی فیچرها و زمان، بسته به منابع و نیاز.

این توضیح ساده و قابل فهم است و عمق فنی را هم نشان می‌دهد بدون این‌که وارد جزئیات بیش از حد شویم.

---

### سؤال ۵ – این سیستم چطور با تیم‌های دیگر یکپارچه می‌شود؟

**پاسخ:**

- ورودی‌ها:  
  - از `ClickHouse` (جدول `kandoo_parameter_nats`) و CSV، که همین الان هم در استک اسنپ وجود دارند.
- خروجی‌ها:
  - مدل‌ها در MLflow ثبت می‌شوند → می‌توانند در سرویس‌های دیگر لود شوند.
  - API پیش‌بینی (`/forecast`) و eventهای NATS برای فراخوانی از سمت سرویس‌های دیگر.

در یک جمله:

> «Pulsar روی شانه‌های زیرساخت فعلی اسنپ سوار می‌شود، نه این‌که چرخ را از اول اختراع کند.»

---

## 9. جمع‌بندی یک‌خطی برای پایان ارائه

> **Pulsar** سرج را از یک مشکل لحظه‌ای و واکنشی، به یک مسأله‌ی قابل پیش‌بینی و قابل مدیریت تبدیل می‌کند؛  
>  با استفاده از داده‌های واقعی اسنپ، لایه‌های مختلف هوش مصنوعی (Feature Engineering زمان‑سری، ElasticNet، CNN و در آینده LSTM/TFN)، ارکستریشن NATS و ثبت کامل در MLflow – آماده برای این‌که روی ناوگان واقعی راننده‌ها اجرا شود و درآمد و تجربه‌ی کاربر را یک پله بالا ببرد.

این متن را می‌توان مستقیم به اسلاید تبدیل کرد، یا به‌عنوان اسکریپت کنار `presentation_UI.html` موقع ارائه استفاده کرد.


